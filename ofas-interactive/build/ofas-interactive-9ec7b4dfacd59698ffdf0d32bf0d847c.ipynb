{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ofas Interactive Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "An interactive Spark application is a long-running application which the user can interact with on runtime. This can be a notebook application or applications running Spark Connect or HiveThrift servers. Spark Connect supports languages such as Python, Scala and Go and potentially all languages supported by gRPC. The HiveThrift server offers SQL interaction through JDBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server side\n",
    "To start a Spark application with SparkConnect server, either run the mainClass SparkConnectServer or enable the SparkConnect plugin. Using the Spark Connect plugin, the application can run other tasks or services while enabling Spark Connect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Connect launch using the SparkConnectServer\n",
    "```json\n",
    "\"mainClass\": \"org.apache.spark.connect.sql.service.SparkConnectServer\",\n",
    "\"deps\": {\n",
    "    \"packages\": [\"org.apache.spark:spark-connect_2.12:3.5.0\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Connect launch using the Spark Connect plugin\n",
    "```json\n",
    "\"sparkConf\": {\n",
    "    \"spark.plugins\": \"org.apache.spark.sql.connect.SparkConnectPlugin\"\n",
    "},\n",
    "\"deps\": {\n",
    "    \"packages\": [\"org.apache.spark:spark-connect_2.12:3.5.0\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client side\n",
    "#### Python library\n",
    "On the client side use the ocean-spark-connect (https://pypi.org/project/ocean-spark-connect) python library to interact with the Spark Connect session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ocean_spark_connect.ocean_spark_session import OceanSparkSession\n",
    "\n",
    "spark = OceanSparkSession.Builder().cluster_id(\"osc-cluster\").appid(\"appid\").profile(\"default\").getOrCreate()\n",
    "spark.sql(\"select random()\").show()\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spotctl\n",
    "Use the spotctl command line tool to open a websocket proxy to the interactive Spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "brew install spotinst/tap/spotctl\n",
    "spotctl configure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "spotctl ocean spark --clusterid osc-cluster --appid appid --profile default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spotctl will start a service on port 15002 (the default Spark Connect port)\n",
    "\n",
    "```sh\n",
    "pyspark --remote sc://localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDBC\n",
    "### Server side\n",
    "To enable JDBC connections to the Spark Application, start the HiveThriftServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch a JDBC server\n",
    "```json\n",
    "\"mainClass\": \"com.netapp.spark.HiveThriftServer\",\n",
    "\"deps\": {\n",
    "    \"packages\": [\"com.netapp.spark:hive:1.2.0\"],\n",
    "    \"repositories\": [\"https://us-central1-maven.pkg.dev/ocean-spark/spark-code-submission-plugin\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client side\n",
    "#### Ofas JDBC driver\n",
    "Use the Ofas JDBC driver with database tool or in your code project. The driver available at the maven coordinates\n",
    "```\n",
    "com.netapp.spark:ofas-jdbc:1.2.0\n",
    "```\n",
    "Use the following public maven repository\n",
    "```\n",
    "https://us-central1-maven.pkg.dev/ocean-spark/spark-code-submission-plugin\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
